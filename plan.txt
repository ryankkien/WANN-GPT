Implementation Plan for Weight-Agnostic GPT-2 (CUDA Adaptation)

Overview: We aim to apply Weight Agnostic Neural Network (WANN) principles to a GPT-2 transformer. In a WANN, the architecture is evolved such that it performs the task even with random or shared weight values
ar5iv.labs.arxiv.org
. For GPT-2, this means evolving transformer architectures (layers, heads, connections) that can handle text classification and generation with minimal (or no) weight training. Below we outline the required components, evolutionary search strategy, evaluation methods, CUDA implementation details, multi-task support, and performance metrics.
Components for a Weight-Agnostic GPT-2 in CUDA

    Base Transformer Architecture: We start from the standard GPT-2 decoder-only Transformer structure, which is comprised of stacked masked self-attention layers and feed-forward layers with residual connections and layer normalization
    trevormcguire.medium.com
    . This provides a strong inductive bias for language tasks. The weight-agnostic twist is that all network weights will be fixed or drawn from a distribution rather than learned.

    Token Representation: Use a fixed token embedding mechanism since we cannot train embeddings in the weight-agnostic regime. One approach is one-hot encoding of input tokens (which the network can process directly), or a fixed random embedding matrix (initialized once and then kept constant). Positional encoding should also be fixed (e.g. sinusoidal encodings) to supply sequence order information without learning.

    Transformer Blocks with Shared Weights: Each Transformer block (self-attention + feed-forward) will be implemented such that all its weight parameters share a single value or come from a small set of values. In the simplest case, a single scalar weight w is used for all connections in the network
    ar5iv.labs.arxiv.org
    . This means every query/key/value projection, every linear layer, etc., uses either w or -w (and 0 for absent connections). We achieve this by generating binary masks for connections as determined by the architecture – if a connection exists, its weight is w (or perhaps w times a fixed scale), and if it’s pruned by the architecture, it’s effectively weight 0. This way, the architecture (which connections exist) encodes the solution, not the specific weight magnitudes. Residual additions have no learned weights (just elementwise sum), and layer normalization can be used with fixed parameters (or omitted for simplicity) to maintain stability.

    Activation Functions as Evolved Components: To infuse more expressive power, each neuron or layer can have an assigned activation function chosen from a diverse set. We will allow not only standard ReLU or GELU, but also sigmoids, tanh, step functions, Gaussians, sinusoidal, etc., similar to the original WANN approach
    ar5iv.labs.arxiv.org
    . This means during architecture search, a mutation can replace a layer’s activation with another (e.g. change a feed-forward activation from ReLU to sine). Supporting multiple activations in CUDA requires implementing device functions for each and a way to select them per neuron (e.g. a switch-case in the activation CUDA kernel or using function pointers). This diversity of activations can help the random-weight network encode richer behaviors.

    Output Heads for Tasks: We design the output layer depending on the task:

        For text generation: we use a vocabulary distribution output (similar to GPT-2’s language modeling head). Since we aren’t training weights, one approach is to tie this output to the (fixed) input embedding matrix – effectively using the embedding matrix (or one-hot decoding) to produce logits for each token. Another approach is to evolve a limited-output mechanism (for example, the architecture might decide to only strongly connect to a subset of output tokens). Implementation-wise, the output logits can be computed by a fixed linear transform from the final layer’s representation to vocabulary size (all weights here could also be set to the shared weight w). This is computationally heavy if vocab is large, but we can optimize it in CUDA by treating it as a matrix multiply with a constant weight matrix.

        For text classification: we can append a special classification node or token. For instance, insert a [CLS] token at the end of the input sequence and have the model produce an output for that token representing the class. The architecture can be set such that the final hidden state of this [CLS] position connects to a small output layer for class scores. In practice, we might implement a set of output neurons (one per class) that take input from the transformer's final layer with fixed weights. The predicted class is the index of the highest activation. (This mirrors WANN’s approach on MNIST, where networks had one output per class and the highest output dictates the class
        ar5iv.labs.arxiv.org
        .) If using the [CLS] token approach, we ensure the causal mask is adjusted so that the [CLS] token can attend to all previous tokens (so it gathers information from the whole sequence). In either case, no training is done on these output weights – they are part of the architecture’s connections that evolution will configure (or they use the shared weight value).

    Shared Weight Parameter Handling: A crucial component is how we inject the same weight value across all connections easily in CUDA. We will maintain a global scalar (or a small vector if using a few distinct values) that represents the weight(s) for the current rollout. During a forward pass, instead of using diverse learned matrices, we use this scalar multiplied by the connection mask. For example, a matrix multiplication in a layer becomes a masked sum: output_j = w * sum(input_i for all i in connection_set_j). We can implement this by generating the mask matrix for connections (1 where a connection exists, 0 where not) and using elementwise multiplication by w within the kernel. Essentially, all linear layers become a series of additions and multiplications by w. This simplifies the weight storage (we only store masks or connection indices) and ensures any weight scaling is uniform.

Evolutionary Architecture Search Strategy

We will employ an evolutionary algorithm (genetic algorithm) to search for effective transformer architectures under the weight-agnostic constraint. The approach is inspired by NEAT (NeuroEvolution of Augmenting Topologies), but we ignore weight optimization and focus purely on evolving topology
ar5iv.labs.arxiv.org
. Key aspects of this strategy:

    Architecture Encoding: We represent the GPT-2-like architecture in a genome. This can be a custom encoding that lists the number of layers, the connectivity of each layer, and activation functions. For example, a genome might encode: N transformer layers, each layer’s number of attention heads, which heads or layers are active or skipped, the feed-forward network size, and activation choices. We also encode any skip-connections or direct input-output connections if allowed. Essentially, the genome is a blueprint for how to instantiate the transformer in the weight-agnostic setting.

    Initial Population: We begin with a population of simple, minimal networks. For instance, the simplest individual could be a network with no hidden layers or only one attention layer with one head and a very small feed-forward section. In WANN, initial networks had no hidden nodes and only a fraction of possible connections
    ar5iv.labs.arxiv.org
    . Analogously, the initial GPT-2 candidates might have 1 transformer block or even direct connections from input to output (bypassing attention) as a baseline. These minimal architectures likely perform at chance level, but they provide a starting point for evolution to add complexity as needed.

    Mutation Operators: We use random mutations to create variation in architectures. The operators will mirror those used in WANNs
    ar5iv.labs.arxiv.org
    , adapted to the transformer context:

        Insert Layer/Node: Add a new transformer layer or sub-component. For example, insert a new self-attention block (with some number of heads) into the sequence of layers, or add a new feed-forward “node” in an existing layer (increasing that layer’s width). This is analogous to WANN’s “insert node” (which split a connection by inserting a new neuron)
        ar5iv.labs.arxiv.org
        . In practice, inserting a layer could be done by taking an existing connection from one layer to the next and replacing it with an intermediate layer. The new layer’s connections (attn heads, etc.) are initialized with the default shared weight mask.

        Add Connection: Introduce a new connection between existing components. In a transformer, this could mean enabling a skip connection that was previously absent (e.g., connecting layer i output directly to layer k > i input), or allowing an attention head to attend to an additional input source. It could also mean connecting an earlier layer directly to the output layer (if we allow such shortcuts). This operator corresponds to adding an edge in the neural graph
        ar5iv.labs.arxiv.org
        . Implementationally, this means updating the mask to include that connection path (for example, setting some mask entries from 0 to 1 for the new links).

        Change Activation: Modify the activation function of a hidden node or layer
        ar5iv.labs.arxiv.org
        . For instance, switch one layer’s activation from GELU to sin or to ReLU. We maintain a set of allowed activations and randomly pick a new one when this mutation occurs. This can drastically change how signals propagate with random weights (e.g., a step function vs. a smooth tanh could change the output behavior even with the same weights).

        Add/Remove Attention Head: (Optional mutation) Increase or decrease the number of attention heads in a layer. For example, adding a head means duplicating an attention mechanism in parallel for that layer (with its connections initialized). Removing a head prunes that part of the architecture (masking out all its connections). This changes the model’s width in the self-attention sublayer.

        Remove Connection/Prune: Though not explicitly listed in original WANN operators, we will also allow removing an existing connection or layer (the inverse of the add operations). This helps simplify architectures and is useful in multi-objective optimization when trying to reduce complexity. For instance, if a layer or head isn’t contributing to fitness, a mutation might remove it (set its connections’ mask to 0).

    These mutations are applied stochastically to copies of top-performing architectures to generate new candidates each generation. Each new architecture is then instantiated (its masks and activation settings applied) and evaluated.

    Evolutionary Loop: We use a standard evolutionary loop:

        Evaluate all individuals in the population on the target task (using the shared-weight evaluation method described in the next section).

        Select top-performing architectures, taking into account both performance and simplicity. We rank or Pareto-sort networks by a fitness measure that balances accuracy (or reward) and complexity. Specifically, we favor architectures that achieve high task performance and have fewer connections (or parameters)
        ar5iv.labs.arxiv.org
        . This can be done via a multi-objective approach (e.g., NSGA-II) where one objective is maximizing mean performance and another is minimizing network size
        ar5iv.labs.arxiv.org
        .

        Reproduce by copying and mutating those top architectures to form a new generation. We may also include crossover (combining parts of two architectures), though topology crossover is tricky; we might instead rely mostly on mutations and carry over some unmodified top performers (elitism).

        Repeat the process for many generations until convergence or a performance threshold is met.

    Multi-Objective Optimization: As noted, we treat performance and model complexity as two key criteria. For example, one could assign a connection cost to penalize large networks
    ar5iv.labs.arxiv.org
    . We evaluate mean performance across weight samples (see below) as the primary fitness, but also track the maximum performance under the best single weight, and the total number of connections. Following the WANN approach, we could use a multi-objective ranking that considers: (a) average performance, (b) best-case performance (in case a network has one optimal weight setting, this captures its ceiling), and (c) network size
    ar5iv.labs.arxiv.org
    . By doing so, if two architectures perform similarly, the one with fewer connections/layers is preferred
    ar5iv.labs.arxiv.org
    . This encourages finding minimal architectures that are inherently capable.

    Topology Constraints: We ensure the evolved networks remain valid transformer-like structures. For example, to preserve the causal structure for generation tasks, any new attention connections must not violate the temporal order (we maintain the causal mask). Similarly, we likely constrain that layers remain in sequence (unless adding skip connections) to keep things interpretable as a Transformer. These constraints are applied during mutation (rejecting or adjusting invalid mutations).

In summary, through evolutionary search we will obtain architectures that “deemphasize weights” – the architecture alone encodes the solution, as demonstrated by WANNs
ar5iv.labs.arxiv.org
. Next, we discuss how to evaluate each candidate during evolution.
Evaluating Architectures with Shared-Weight Sampling

Evaluating a candidate architecture involves testing how well it performs with random shared weights, since we do not train the weights. We use a shared weight sampling strategy as in Gaier & Ha’s WANN
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
:

    Single Shared Weight Rollouts: For each architecture, we assign a single scalar weight value w to all connections during a forward pass (all masks * 1 are effectively scaled by w). We then run the network on the task. Instead of training weights, we sample w from a predefined distribution each time and measure the network’s performance. For instance, one rollout might set w = 1.2 everywhere, another might set w = -0.5, etc., covering a range of positive and negative values
    ar5iv.labs.arxiv.org
    . This tests whether the structure can succeed regardless of the exact weight magnitude.

    Multiple Weight Samples (Ensemble Evaluation): We evaluate the expected performance of an architecture by trying several weight values and aggregating results
    ar5iv.labs.arxiv.org
    ar5iv.labs.arxiv.org
    . For example, use a fixed set of weights like w ∈ {−2.5, −1.0, −0.5, 0.5, 1.0, 2.5} (excluding 0 which would zero-out all signals, and avoiding extremely large magnitudes that saturate activations
    ar5iv.labs.arxiv.org
    ). For each such w, run the network forward on a validation dataset and record the performance:

        For a classification task, compute the accuracy (or other relevant score) with that weight.

        For a generation task, compute the language model perplexity or next-token prediction loss with that weight.
        We then take the mean performance across these weight trials as the fitness of the architecture
        ar5iv.labs.arxiv.org
        . This mean reflects how robust the architecture is to weight changes. A high average accuracy or low average perplexity means the network consistently performs well for any weight in the sample range – a hallmark of weight-agnostic success.

    Best-Weight Performance: We may also note the best performance among the sampled weights as a secondary metric
    ar5iv.labs.arxiv.org
    . This indicates the ceiling of the architecture if one were to tune the weight a bit. However, our primary goal is a design that doesn’t require precise tuning, so the mean (expected) performance is key for selection
    ar5iv.labs.arxiv.org
    .

    Ensemble of Weight Instances: In addition to using multiple weight samples for evaluation, we can leverage them at inference time for more robust predictions. For instance, an evolved model could be run with several different weight values in parallel, and their outputs combined:

        For classification, treat each weight value’s prediction as a “vote” and use majority voting or average the output probabilities from each instance to make a final ensemble decision. This can smooth out any variability due to a particular weight value and improve accuracy.

        For generation, one could generate multiple continuations using different weight seeds and then select or rank outputs (though this is more for quality evaluation than a single output).

    This ensemble approach is practically feasible because each individual network evaluation is fast on GPU (see next section) and the weights are not trained – they are just random seeds. It essentially treats the random weight as a source of ensemble diversity. During evolution, however, we primarily use the average over weights as a fitness measure
    ar5iv.labs.arxiv.org
    , which implicitly encourages architectures that internally ensemble well (i.e., yield consistent outputs across weight variations).

    Evaluation Data and Procedure: We will have a validation dataset for each task to evaluate performance quickly (distinct from any test set used for final reporting). For classification, we feed in a batch of labeled examples through the network (with a given w) and compute accuracy. For language modeling, we compute perplexity by feeding in text and using the network’s output distribution to calculate log-likelihood of the next tokens. All these computations are done on GPU for speed. Notably, we implement the evaluation as batched operations: for a given architecture and weight w, we can evaluate multiple examples in parallel (the batch dimension in CUDA) to get that rollout’s score efficiently.

    Parallel Weight Evaluation: To further speed up, we can evaluate multiple weight samples in one go. One way is to use an extra dimension in the batch for weight samples. For example, given a batch of input examples, duplicate it across N weight values, and in the forward pass, have each “slice” of the batch use a different w. Since the network operations are mostly linear and element-wise, we can broadcast the weight scalar as a vector of length N and perform the operations for all weight instances simultaneously. This effectively treats weight as another parallel axis, leveraging GPU parallelism. After the forward pass, we compute N outcomes and average them. This reduces overhead of launching separate kernels for each weight. We must ensure memory can handle the expanded batch, but for moderate N (say 5-10 weight samples) it’s manageable.

    Fitness Calculation: After obtaining the mean performance for an architecture, that value is used as the fitness in the evolutionary selection. If using multi-objective, the complexity (connection count) is also considered as described. The evaluation procedure is repeated each generation for all new architectures.

In essence, this evaluation strategy ensures we “measure the expected performance” of each architecture under random weights
ar5iv.labs.arxiv.org
. Architectures that maintain decent performance over a wide range of w are favored, aligning with the weight-agnostic principle of not relying on precise weight tuning
ar5iv.labs.arxiv.org
.
Key CUDA Kernels and Modules for Fast Evaluation

A core part of this plan is implementing the forward passes of candidate networks efficiently on the GPU. We outline the main CUDA kernels/modules needed for rolling out (evaluating) the transformer and how to optimize them:

    Token Embedding & Input Preparation: If using one-hot token inputs, the embedding lookup is essentially picking the one-hot vector (which can be done by indexing or a scatter operation). If using a fixed embedding matrix, we implement the lookup as a matrix-vector multiply. On GPU, we can use a simple kernel that reads token IDs and writes out the corresponding vector (the matrix can be in constant memory or global memory). This is a memory-bound operation, but straightforward to parallelize: each token’s embedding can be fetched by multiple threads reading contiguous memory. We will also add positional encodings to token embeddings, which can be computed on the fly in a CUDA kernel (applying sin/cos formulas for each position index) or precomputed and stored.

    Masked Multi-Head Attention Kernel: Multi-head self-attention is the most compute-intensive part. We will implement it with GPU acceleration, possibly fusing multiple steps:

        Linear projections (Q, K, V) – Normally this is Q = X * Wq, K = X * Wk, V = X * Wv. Here, Wq, Wk, Wv are all filled with the shared weight w wherever the architecture mask has a connection. Instead of performing separate matrix multiplies, we can exploit the structure: if all weights are w, each output component of Q is basically w * (sum of a subset of input features). We can implement the projection as a sparse-like operation: for each output dimension, add up inputs that are connected. However, since many connections might be present, it could degrade to a dense operation. A practical approach is to use cuBLAS GEMM for these projections, supplying a matrix of 0/1 (mask) multiplied by w. We can scale the result by w afterward (since w is a scalar) to avoid modifying the matrix data for each weight. This yields Q, K, V efficiently. Alternatively, writing a custom kernel to do all three projections in one go could reduce memory reads (load X once, compute XWq, XWk, X*Wv together), but this is akin to a fused GEMM which is complex. Initially, standard matrix multiply libraries with proper strides for heads should suffice.

        Attention scores computation – Compute the dot-product attention scores S = Q * K^T for each head. This results in a matrix of shape [seq_len x seq_len] per head. We apply the causal mask here: in the kernel, when computing score for query position i and key position j, if j > i (future token), we set that score to a very large negative number (to zero out its softmax). This masking can be done by branching in the kernel or by adding a mask matrix before softmax. We parallelize this computation across threads/blocks (tiled matrix multiplication). Memory optimization is key: computing the full QK^T for long sequences can be memory-heavy.

        Softmax and weighted sum – We apply softmax to each row of the score matrix to get attention weights, then multiply by V to get the attention output. We can leverage optimized kernels here. A modern approach is to use FlashAttention, which fuses these operations to avoid materializing large intermediate matrices
        medium.com
        . FlashAttention breaks the sequence into blocks and computes attention in tiles entirely in GPU shared memory, performing the softmax on the fly
        medium.com
        medium.com
        . We can implement our own fused kernel or use an existing library if available. The key idea is to combine Q*K^T, masking, exponential and normalization, and the final multiplication with V in one kernel launch to minimize memory traffic
        medium.com
        . This is especially beneficial if sequence length is large. Since our focus is architecture search (which might use moderate sequence lengths for feasibility), we can start with simpler kernels and optimize later with FlashAttention-style techniques for speed.

    In summary, the attention module on CUDA will involve heavy use of matrix operations and softmax. We ensure to use floating-point math optimized for GPU (possibly FP16 with Tensor Cores if precision tolerates, to speed up GEMMs). Each head’s computation can be parallelized, and heads themselves are independent (we might parallelize over heads as well). After obtaining each head’s output, we merge them (concatenate and project if needed – though GPT-2 does a linear combination of heads outputs, which again would be a fixed mask matrix multiply by w).

    Feed-Forward Layer Kernel: Each transformer layer has a feed-forward network (typically two linear layers with an activation in between). For weight agnostic GPT-2, these linear layers are also filled with the shared weight and pruned by architecture. We implement the first linear transform similar to the projections above (a GEMM or custom kernel), then apply the non-linear activation, then the second linear transform. We can fuse parts of this:

        The matrix multiply for the first layer (projecting from model dimension to hidden dimension) can use cuBLAS. The activation function (which could be ReLU, tanh, sin, etc.) is then applied elementwise – we will write a CUDA kernel that takes the matrix of pre-activations and applies the chosen activation for each neuron. Because different layers might have different activation types (as evolved), we need a strategy: one approach is to implement a generic activation kernel that takes an array of function identifiers per neuron or per layer and uses a switch at runtime. Alternatively, we compile separate versions or use templates for each activation and dispatch accordingly. Given the small variety of functions, a straightforward if/else on an activation ID inside the kernel is fine (the cost is minimal compared to memory transfers).

        The second linear transform (from hidden back to model dimension) is another matrix multiply. Again, since weights are uniform, it’s a lot of summing and scaling. We might combine this with the activation in one kernel: for example, each thread block could load a chunk of the input, compute the first matmul, apply activation, and immediately multiply by the second layer’s weights to produce the output chunk. This would save writing out the intermediate activation results to global memory. Essentially it becomes a fused two-layer MLP kernel. This is an optimization that can be attempted if profiling shows the activation write-read is a bottleneck. For initial implementation, keeping them separate (matmul -> activation kernel -> matmul) is simpler and leverages well-tested libraries for the heavy lifts.

    Residual Connection and Layer Norm: After each sub-layer (attention or feed-forward), GPT-2 adds the input back to the output (residual add) and applies layer normalization. Residual add is trivial to implement on GPU: an elementwise vector addition (which can be done by a simple kernel or even using thrust/BLAS axpy). Layer normalization computes mean and variance for the vector and normalizes. We will implement this with a CUDA kernel that does reduction (to compute mean and variance across the features) and then scales the output. Since we want weight agnostic behavior, we will not train the layer norm’s scale and bias; instead, we can either fix them to 1 and 0 respectively, or include them as part of the architecture parameters (potentially evolvable, but likely keep fixed). The layer norm kernel will thus subtract the mean and divide by std for each feature. We need to be careful with floating point precision here if using FP16 (might need to do norm in FP32 for stability). If performance is an issue, we could rely on cuDNN’s LayerNorm routine, but writing a custom one gives flexibility to ensure it’s done in one pass for the whole batch.

    Output Computation Kernels:

        For classification, after the final transformer layer (or directly from inputs if no hidden layer), we might have a small set of output neurons (one per class). We implement a kernel that takes the final hidden state corresponding to the [CLS] token or a pooled representation of the sequence, and computes class scores. If the architecture directly connects some hidden units to outputs with weight w, that is just a sum of those units (scaled by w). Essentially, class score = w * sum(selected hidden features). This is a dot-product that a single CUDA thread block can handle per class (since number of classes is usually small). After obtaining scores for each class, we would normally apply a softmax to get probabilities, but for choosing the label, an argmax is sufficient. The softmax and cross-entropy loss (for evaluating accuracy) can be computed on GPU as well if needed (though accuracy just needs argmax). We can use Thrust or a small kernel to compute the max index per output vector.

        For generation, the output is a probability distribution over a large vocabulary. Computing this naively is a matrix multiply of size [vocab_size x model_dim]. GPT-2 usually uses a weight matrix of that shape. In our case, that weight matrix could be fixed (e.g., equal to the transpose of the embedding matrix if tying weights). We will implement the logits calculation as a matrix multiply on GPU. Given the potentially large vocab (50k+ for GPT-2), this is heavy – but since we might work with smaller vocab in experiments (or we accept the cost), using a library GEMM (like cuBLAS) is fine. We again exploit that weights are largely the same value: the multiplication effectively sums the components of the hidden state for each vocab item that is connected. If every vocab token is connected to every hidden unit with weight w, the logit for token t is w * sum(hidden) + b_t (if bias allowed, but we likely omit bias for simplicity). That means every logit differs only by which hidden units are summed (if all are connected, then all logits are just w * sum(hidden) making them almost identical – which suggests in a weight-agnostic scenario we might rely on architecture to drop different connections to differentiate tokens). To allow differentiation, we could evolve a sparsified output layer where each token’s logit is connected to a subset of neurons. Implementation-wise, we can precompute a mask for the output layer of shape [vocab, hidden] indicating which connections exist, and then do a masked matmul (or compute each token’s logit by summing over its connected hidden features). This can be parallelized by assigning threads to tokens (each thread sums over hidden units for one token, using shared memory to accumulate partial sums). The result is a logits array. We then apply a softmax across the vocab. Softmax for large vocab can also be optimized but using cuDNN or a custom kernel that does an efficient reduction (find max, subtract, exp, sum, divide). For perplexity calculation, we would then take the probability of the correct next token and accumulate the log. All these operations are routine in deep learning frameworks and can be mirrored in custom CUDA code for full control.

    Random Weight Sampling: Although conceptually we run multiple forward passes for different w, implementing that in CUDA can leverage batch processing as mentioned. We might have a kernel that simply multiplies all active connections by the scalar w at the start of a rollout (effectively scaling the mask). However, since scaling by w is linear, we can often fold it into other operations (for example, when we launch the GEMM for a linear layer, we can pass alpha = w to scale the result by w instead of doing it separately). For multiple w in parallel, we may duplicate the input and do one pass per w simultaneously. Each kernel will thus possibly get an extra dimension (for weight index) to iterate over.

    Parallelism and Memory: Each evaluation will involve processing a batch of input sequences. We will utilize block-level and thread-level parallelism for matrix ops, and ensure memory coalescing (store weight masks and inputs in contiguous arrays). If population size is large, we might run multiple network evaluations concurrently on GPU using streams (one stream per individual) or sequentially if each is heavy. The main computational burden is the forward pass, which our kernels aim to optimize. There is negligible backprop since no training, so we don’t run gradients – this spares a lot of computation and memory overhead, meaning we can devote all GPU capacity to forward inference.

In summary, by implementing custom CUDA kernels for each part of the forward pass (embedding, attention, feed-forward, normalization, output), we can achieve fast throughput. Techniques like kernel fusion (as in FlashAttention for the attention part) reduce memory access costs
medium.com
, which is important when evaluating many architectures and weight samples. The goal is to make each rollout of a candidate as fast as possible so that the evolutionary search (which may require thousands of forward passes) is tractable.
Supporting Text Classification and Text Generation Tasks

Our framework will support two modes of operation – one for classification and one for generation – often using the same underlying architecture but with minor adjustments in how inputs/outputs are handled and how we interpret the network’s output:

    Classification Mode: In this mode, the model reads an entire input text (e.g., a sentence or document) and produces a class label. To accomplish this with a GPT-2 style architecture, we have a couple of design choices:

        Using a Classification Token: We can append a special [CLS] token to the input sequence. During the forward pass, this token (at the end) will attend to all previous tokens (we ensure the mask allows it to see earlier positions). The final hidden state corresponding to [CLS] is taken as the representation of the whole sequence. The architecture can be set to have direct connections from this [CLS] state to the output label neurons (with shared weight). Essentially, the network is tasked with routing information from the sequence into the [CLS] node using attention, and then from [CLS] to the correct label output. This mimics how BERT does classification, except here nothing is trained – the architecture must create the right inductive biases.

        Direct Output Nodes: Alternatively, we designate a small set of output nodes (one per class) that directly connect to the transformer’s final layer (or even to certain tokens in the input). For example, for sentiment classification (positive/negative), we could have two output neurons. The architecture might evolve to connect certain words or patterns to activate one output more than the other via some logic of connections and activations. This is more akin to a feed-forward network attached on top of the transformer layers. With random weights, this essentially forms a random classifier, but evolution will shape which connections exist (e.g., maybe the presence of certain keywords strongly activates the positive output neuron through a chain of connections).

        Attention Usage: During classification, we might allow the transformer to use full attention (bidirectional) since the whole sequence is given at once. In GPT-2, attention is typically causal (one-directional). If we stick strictly to GPT-2 architecture, the attention will be causal, which means tokens can only attend to earlier tokens. However, the [CLS] token hack effectively allows one position to aggregate information from all others even in a causal setting (because [CLS] comes last and can attend to all previous). This way, we don’t need to modify the masking behavior and still get a global view for the classifier.

        CUDA Implementation: The same kernels described earlier apply, with the only difference being the output layer deals with a small number of classes. We’ll run the forward pass on the entire input in one go (since for classification we aren’t generating token by token). The output is a vector of class scores, from which we pick the argmax for prediction. During evolution, fitness might be the classification accuracy on a validation set (percentage of examples correctly classified).

    Generation (Language Modeling) Mode: In this mode, the model functions as a standard language model, generating text or predicting next tokens:

        Causal Masking: We enforce the causal mask in self-attention so that at token position i, the model only attends to tokens <i. This is inherently part of GPT-2 and we keep it to evaluate perplexity properly.

        Evaluation via Perplexity: For a given test sequence, we can evaluate how well the model predicts it by feeding the sequence and at each position computing the probability it assigns to the actual next token. For example, to get perplexity, we do a single forward pass: input the first N-1 tokens, and have the model produce probability distribution for token 2..N. Because of masking, token j’s prediction only used tokens < j. We calculate the log-likelihood of the actual token at each position from the model’s output distribution. Summing these gives a total log-likelihood, and from that perplexity is derived. This can be done in a vectorized way on CUDA, utilizing the output softmax kernel over the vocabulary for each position. We then average exponentiated loss to get perplexity. A lower perplexity means the model (architecture) is better at modeling the text.

        Text Generation: Besides perplexity, we can also test the model’s qualitative generation by actually sampling tokens. To do this, we start with a prompt (or empty) and iteratively feed it through the model to get a next-token distribution, sample one token, append it, and repeat. This requires running the model step-by-step (or implementing an efficient stateful generation that keeps track of key/value history to avoid recomputation). Given that our model is untrained, the outputs may not be meaningful, but this procedure can demonstrate if the architecture at least generates non-trivial patterns (for instance, some architectures might default to repeating certain tokens or producing balanced parentheses if that structure is wired in).

        Task Variation: We can evaluate generation on standard language modeling datasets or even on simpler tasks like regenerating a seen sequence (to see if the network can memorize input). But primarily, perplexity on natural text will be our metric.

        CUDA Implementation: The forward pass for generation is identical to the one described in the kernels section. If doing iterative generation, we either run the full sequence each time (inefficient) or maintain the attention state incrementally. For simplicity, during evolution evaluation we likely won’t do long iterative generation; we’ll stick to perplexity calculation on fixed datasets (which is a single forward per example). The GPU kernels will handle sequence batches, and the masking ensures no information leakage from future tokens.

    Unified vs. Separate Architectures: We can either evolve one architecture for both tasks or treat them separately:

        Evolving a single architecture to perform both a classification and a generation task would require a multi-objective fitness (e.g., optimizing both accuracy and perplexity at once). This is advanced and might result in a compromise solution. If attempted, the architecture would need to include both the classification head and the language model head and be evaluated on both types of data.

        More straightforward is to run the evolution twice: once on a classification task (yielding a specialized architecture for classification), and once on a language modeling task (yielding an architecture for generation). Our implementation plan supports both; you simply switch the evaluation criterion and possibly allow slight changes (like adding the [CLS] token for the classification case).

        If using one architecture for both, we ensure the architecture includes the necessary components for each (e.g., it has the classification output neurons and also uses causal masking for LM). The evaluation would then have to alternate or combine tasks. For clarity, we likely separate them, but the infrastructure (CUDA kernels, mutation operations) is common.

    Ensuring Inductive Bias for Each Task: The design of the search space can include task-specific biases. For generation, having positional encodings and causal attention is crucial (so we won’t evolve those away). For classification, having some mechanism to aggregate sequence info (like the [CLS] token approach or an attention head that effectively averages inputs) will be important. We will monitor that during evolution – e.g., if an architecture drops all attention layers, it might fail to capture long-range dependencies, resulting in poor perplexity or accuracy; such architectures will get low fitness and be discarded.

Overall, the system will be flexible in supporting both use cases. The same low-level CUDA code runs the forward pass, but the setup and evaluation metrics differ. A practitioner can plug in a text classification dataset or a language modeling dataset and run the evolutionary search with minimal changes, leveraging the weight-agnostic architecture search in either scenario.
Performance Benchmarking and Analysis

After implementing and evolving weight-agnostic GPT-2 models, we will analyze their performance using several metrics and criteria:

    Classification Accuracy: For classification tasks, measure the final evolved model’s accuracy on a held-out test set. This is the fraction of examples for which the predicted label (via the weight-agnostic model) matches the true label. We expect these architectures to outperform chance significantly (prior WANN work showed random-weight networks can reach well above chance on tasks like MNIST
    ar5iv.labs.arxiv.org
    ). We will compare the accuracy to baselines (e.g., a trivial predictor or a fully-trained GPT-2 fine-tuned on the task) to understand the gap.

    Language Model Perplexity: For generation tasks, report the perplexity of the evolved model on a test corpus. Perplexity is a standard metric indicating how well a model predicts test data (lower is better). Even though our models aren’t trained, a good architecture should have a lower perplexity than a random uninformed model (which would have perplexity equal to the vocabulary size if guessing uniformly). We might also look at token prediction accuracy (how often the top predicted token is correct) as an intuitive measure, but perplexity captures the overall probability distribution quality.

    Model Size and Complexity: We record the size of the evolved architectures: number of layers, number of attention heads, and total number of parameters/connections. This is important to verify that the evolutionary search indeed finds minimal or efficient solutions
    ar5iv.labs.arxiv.org
    . We will benchmark how large or small the weight-agnostic model ended up compared to a standard GPT-2. For example, we might find an architecture with only a few layers and sparse connections achieves reasonable performance. Complexity can be measured in description length or simply count of non-zero weights (connections). We might also evaluate the inference speed of the model (since it’s smaller or has a certain structure, does it run faster than a full GPT-2 on GPU?). This is a secondary consideration but useful if weight-agnostic models turn out to be more efficient.

    Robustness Across Weight Values: A key hallmark of WANNs is that the model works for a range of weight values, not a finely-tuned weight. To assess this, we will analyze the model’s sensitivity to the weight parameter:

        We can plot the model’s performance (accuracy or perplexity) as a function of the shared weight w. For example, fix the architecture and vary w from -3 to +3 and see how the metric changes. Ideally, the performance stays relatively flat or at least has a broad plateau around the optimal range
        ar5iv.labs.arxiv.org
        . If the performance sharply peaks at a specific w, the model is less truly weight-agnostic.

        We also calculate the variance of performance across the weight samples we used. During evolution we used mean performance; now we can report that mean and the standard deviation. A low stddev means the architecture is consistent (robust), whereas a higher stddev means it was sometimes good, sometimes bad depending on weight. We prefer the former. In WANN, they optimized for expected (mean) reward explicitly
        ar5iv.labs.arxiv.org
        , so our final models should demonstrate stability.

        Additionally, we might check the best-case vs average-case performance. If the best single-weight performance is much higher than the average, it indicates the architecture could do very well if we happened to pick the “right” weight. We could consider a simple fine-tuning of the single shared weight as a post-process (since that’s just one scalar to tune on a validation set). Reporting the best-case performance gives an upper bound on what the architecture could achieve if that weight were tuned. However, if the average is close to the best-case, it means the architecture is naturally robust.

    Task-Generalization and Ensemble Performance: If applicable, test the evolved model on data it wasn’t explicitly evolved on. For example, if we evolved it on one text genre, does it also get above-chance performance on another? This checks if the architecture’s inductive biases are somewhat general or over-specialized. We can also evaluate the effect of the ensemble method: does combining multiple weight instances (as described earlier) improve accuracy or perplexity versus using just one instance? We expect some improvement due to averaging out random fluctuations. We will quantify that (e.g., single weight vs ensemble of 5 weights accuracy).

    Comparative Benchmarks: Compare the weight-agnostic model’s performance to:

        A fully-trained GPT-2 of similar size (just to see how far off we are from a trained solution).

        A randomly initialized GPT-2 of the same architecture (without weight agnostic design). For instance, if our evolved model has 4 layers and 4 heads, we can take a standard 4-layer transformer with random weights (drawn from typical initialization like Xavier) and see its accuracy/perplexity. This tells us whether the evolved architecture is truly special or if any random small transformer would have done similarly. Prior work suggests evolved WANN architectures far outperform random architecture with random weights
        ar5iv.labs.arxiv.org
        .

        Possibly compare to the Supermasks idea (finding subnetworks in a large random network) if relevant: This isn’t directly our approach, but it’s another way to get untrained networks to perform tasks. It would be interesting to see if our evolved tiny network can match a subnetwork of a large network.

    Qualitative Analysis: For generation, we might present a few example outputs from the weight-agnostic model (with a fixed random seed). Are the outputs completely nonsensical, or do they show some structure (like repetitive patterns, or basic grammar)? Since the model isn’t trained, we expect mostly incoherent text, but the architecture might hard-wire some behaviors (e.g., some heads might copy input words to output, resulting in output that echoes input or common tokens). Any such observed behaviors can give insight into what the architecture is doing. For classification, we could inspect which features (words or positions) the architecture seems to use – e.g., if it solved sentiment, did it pick up on certain trigger words via direct connections? We can do this by tracing the activated paths in the network for different inputs.

    Ablation Studies: We can take the final architecture and disable certain components to see their impact (since we can toggle connection masks). For example, if the architecture has 3 heads, try turning off one head to see if accuracy drops – this identifies which heads are crucial. Similarly, test the architecture with weight = 0 (which would make it output either zeros or a default class) to confirm it fails as expected (since 0 weight passes no info). This helps verify that it’s the connections and not some trivial effect that caused success.

The above results will allow us to benchmark performance comprehensively. We will focus on metrics like accuracy and perplexity to quantify task performance, model size to see complexity, and robustness across weights to ensure we indeed achieved weight-agnostic behavior
ar5iv.labs.arxiv.org
. An ideal outcome is an architecture that is compact (small number of connections), robust (works for any weight ~1), and achieves reasonably good accuracy/perplexity on the tasks – demonstrating that even without weight training, the right architecture can encode non-trivial solutions
ar5iv.labs.arxiv.org
. We will document such findings and any trade-offs (e.g., perhaps the evolved model needs to be somewhat larger to compensate for no learned weights, etc.). This analysis will guide future improvements and confirm the viability of weight-agnostic neural network design in complex domains like language modeling.

Sources: The methodology is informed by the original WANN paper by Gaier & Ha
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
, which demonstrated evolving minimal networks that perform tasks with random weights. We also leverage known details of GPT-2’s architecture
trevormcguire.medium.com
and state-of-the-art GPU optimization techniques like fused attention kernels
medium.com
to ensure the implementation is practical.